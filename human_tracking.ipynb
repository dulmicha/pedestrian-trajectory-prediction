{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/england.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a YOLO model \n",
    "model = YOLO('yolov8x.pt')\n",
    "\n",
    "#geting names from classes\n",
    "dict_classes = model.model.names\n",
    "dict_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_frame(frame, scale_percent):\n",
    "    \"\"\"Function to resize an image in a percent scale\"\"\"\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "\n",
    "    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "\n",
    "\n",
    "def filter_tracks(centers, patience):\n",
    "    \"\"\"Function to filter track history\"\"\"\n",
    "    filter_dict = {}\n",
    "    for k, i in centers.items():\n",
    "        d_frames = i.items()\n",
    "        filter_dict[k] = dict(list(d_frames)[-patience:])\n",
    "\n",
    "    return filter_dict\n",
    "\n",
    "\n",
    "def update_tracking(centers_old,obj_center, thr_centers, lastKey, frame, frame_max):\n",
    "    \"\"\"Function to update track of objects\"\"\"\n",
    "    is_new = 0\n",
    "    lastpos = [(k, list(center.keys())[-1], list(center.values())[-1]) for k, center in centers_old.items()]\n",
    "    lastpos = [(i[0], i[2]) for i in lastpos if abs(i[1] - frame) <= frame_max]\n",
    "    # Calculating distance from existing centers points\n",
    "    previous_pos = [(k,obj_center) for k,centers in lastpos if (np.linalg.norm(np.array(centers) - np.array(obj_center)) < thr_centers)]\n",
    "    # if distance less than a threshold, it will update its positions\n",
    "    if previous_pos:\n",
    "        id_obj = previous_pos[0][0]\n",
    "        centers_old[id_obj][frame] = obj_center\n",
    "    \n",
    "    # Else a new ID will be set to the given object\n",
    "    else:\n",
    "        if lastKey:\n",
    "            last = lastKey.split('D')[1]\n",
    "            id_obj = 'ID' + str(int(last)+1)\n",
    "        else:\n",
    "            id_obj = 'ID0'\n",
    "            \n",
    "        is_new = 1\n",
    "        centers_old[id_obj] = {frame:obj_center}\n",
    "        lastKey = list(centers_old.keys())[-1]\n",
    "\n",
    "    \n",
    "    return centers_old, id_obj, is_new, lastKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configurations\n",
    "#Verbose during prediction\n",
    "verbose = False\n",
    "# Scaling percentage of original frame\n",
    "scale_percent = 100\n",
    "# model confidence level\n",
    "conf_level = 0.5\n",
    "# Threshold of centers ( old\\new)\n",
    "thr_centers = 30\n",
    "#Number of max frames to consider a object lost \n",
    "frame_max = 15\n",
    "# Number of max tracked centers stored \n",
    "patience = 100\n",
    "# ROI area color transparency\n",
    "alpha = 0.2\n",
    "#-------------------------------------------------------\n",
    "# Reading video with cv2\n",
    "video = cv2.VideoCapture(path)\n",
    "\n",
    "# Objects to detect Yolo\n",
    "class_IDS = [0] \n",
    "# Auxiliary variables\n",
    "centers_old = {}\n",
    "\n",
    "obj_id = 0 \n",
    "end = []\n",
    "frames_list = []\n",
    "count_p = 0\n",
    "lastKey = ''\n",
    "print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
    "\n",
    "\n",
    "# Original informations of video\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print('[INFO] - Original Dim: ', (width, height))\n",
    "\n",
    "# Scaling Video for better performance \n",
    "if scale_percent != 100:\n",
    "    print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print('[INFO] - Dim Scaled: ', (width, height))\n",
    "    \n",
    "\n",
    "#-------------------------------------------------------\n",
    "### Video output ####\n",
    "video_name = 'result.mp4'\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "output_video = cv2.VideoWriter(tmp_output_path, \n",
    "                               cv2.VideoWriter_fourcc(*VIDEO_CODEC), \n",
    "                               fps, (width, height))\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Executing Recognition \n",
    "for i in range(int(video.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "    \n",
    "    # reading frame from video\n",
    "    _, frame = video.read()\n",
    "    \n",
    "    #Applying resizing of read frame\n",
    "    frame  = resize_frame(frame, scale_percent)\n",
    "#     frame  = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # area_roi = [np.array([ (1250, 400),(750,400),(700,800) ,(1200,800)], np.int32)]\n",
    "    # ROI = frame[390:800, 700:1300]\n",
    "    # area_roi = [np.array([ (0, 0),(0,frame.shape[1]),(frame.shape[0],frame.shape[1]) ,(frame.shape[0],0)], np.int32)]\n",
    "    # ROI = frame[50:200, 100:170]\n",
    "\n",
    "    ROI = frame\n",
    "  \n",
    "    if verbose:\n",
    "        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Getting predictions\n",
    "    y_hat = model.predict(ROI, conf = conf_level, classes = class_IDS, device = 0, verbose = False)\n",
    "    \n",
    "    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\n",
    "    boxes   = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "    conf    = y_hat[0].boxes.conf.cpu().numpy()\n",
    "    classes = y_hat[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "\n",
    "    # Storing the above information in a dataframe\n",
    "    positions_frame = pd.DataFrame(y_hat[0].boxes.data.cpu().numpy().astype('float'), columns = ['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "    \n",
    "    #Translating the numeric class labels to text\n",
    "    labels = [dict_classes[i] for i in classes]\n",
    "    \n",
    "    \n",
    "    # For each people, draw the bounding-box and counting each one the pass thought the ROI area\n",
    "    for ix, row in enumerate(positions_frame.iterrows()):\n",
    "        # Getting the coordinates of each vehicle (row)\n",
    "        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')\n",
    "        \n",
    "        # Calculating the center of the bounding-box\n",
    "        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)\n",
    "        \n",
    "        #Updating the tracking for each object\n",
    "        centers_old, id_obj, is_new, lastKey = update_tracking(centers_old, (center_x, center_y), thr_centers, lastKey, i, frame_max)\n",
    "        \n",
    "\n",
    "        #Updating people in roi\n",
    "        count_p+=is_new\n",
    "        \n",
    "        # drawing center and bounding-box in the given frame \n",
    "        cv2.rectangle(ROI, (xmin, ymin), (xmax, ymax), (0,0,255), 2) # box\n",
    "        for center_x,center_y in centers_old[id_obj].values():\n",
    "            cv2.circle(ROI, (center_x,center_y), 5,(0,0,255),-1) # center of box\n",
    "        \n",
    "        #Drawing above the bounding-box the name of class recognized.\n",
    "        cv2.putText(img=ROI, text=id_obj+':'+str(np.round(conf[ix],2)),\n",
    "                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.8, color=(0, 0, 255),thickness=1)\n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "    # #drawing the number of people\n",
    "    # cv2.putText(img=frame, text=f'Counts People in ROI: {count_p}', \n",
    "    #             org= (10,10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "    #             fontScale=0.2, color=(255, 0, 0), thickness=1)\n",
    "    \n",
    "    # Filtering tracks history\n",
    "    centers_old = filter_tracks(centers_old, patience)\n",
    "    # if verbose:\n",
    "    #     print(contador_in, contador_out)\n",
    "    \n",
    "    #Drawing the ROI area\n",
    "    # overlay = frame.copy()\n",
    "  \n",
    "    # cv2.polylines(overlay, pts = area_roi, isClosed = True, color=(255, 0, 0),thickness=2)\n",
    "    # cv2.fillPoly(overlay, area_roi, (255,0,0))\n",
    "    # frame = cv2.addWeighted(overlay, alpha,frame , 1 - alpha, 0)\n",
    "\n",
    "    #Saving frames in a list \n",
    "    frames_list.append(frame)\n",
    "    #saving transformed frames in a output video formaat\n",
    "    output_video.write(frame)\n",
    "\n",
    "    \n",
    "#Releasing the video    \n",
    "output_video.release()\n",
    "\n",
    "\n",
    "####  pos processing\n",
    "# Fixing video output codec to run in the notebook\\browser\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    \n",
    "subprocess.run(\n",
    "    [\"ffmpeg\",  \"-i\", tmp_output_path,\"-crf\",\"18\",\"-preset\",\"veryfast\",\"-hide_banner\",\"-loglevel\",\"error\",\"-vcodec\",\"libx264\",output_path])\n",
    "os.remove(tmp_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official ultralytics example\n",
    "\n",
    "https://docs.ultralytics.com/modes/track/#plotting-tracks-over-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"data/deer_mountain.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Store the track history\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "resolution = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "# Store the detected frames indices and speeds\n",
    "frames_list = defaultdict(lambda: [])\n",
    "speeds = defaultdict(lambda: [])\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True, classes=[0])\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        boxes = results[0].boxes.xywh.cpu()\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot(conf=False)\n",
    "\n",
    "        # Plot the tracks\n",
    "        for box, track_id in zip(boxes, track_ids):\n",
    "            x, y, w, h = box\n",
    "            track = track_history[track_id]\n",
    "            track.append((float(x), float(y)))  # x, y center point\n",
    "            if len(track) > 30:  # retain 90 tracks for 90 frames\n",
    "                track.pop(0)\n",
    "\n",
    "            frames = frames_list[track_id] # indices of frames in which the object was detected\n",
    "            frames.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "            if len(frames) > 2:\n",
    "                frames.pop(0)\n",
    "\n",
    "            # Calculate the speed\n",
    "            if len(track) > 1:\n",
    "                speed = np.linalg.norm(np.array(track[-1]) - np.array(track[-2])) * fps / (frames[-1] - frames[-2])\n",
    "                speeds[track_id] = speed\n",
    "\n",
    "                # Draw the speed in box title\n",
    "                cv2.putText(annotated_frame, f\"{speed:.2f} px/s\", (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 1)\n",
    "\n",
    "            # Draw the tracking lines\n",
    "            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjustment to our task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videos obtained from: https://jelenia-gora.webcamera.pl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "\n",
    "def download_video(record_duration=10):\n",
    "    stream_url = \"https://hoktastream4.webcamera.pl/jeleniagora_cam_73a10c/jeleniagora_cam_73a10c.stream/playlist.m3u8\"\n",
    "\n",
    "    output_file = f\"data/{time.strftime('%Y%m%d-%H%M%S')}_{record_duration}s.mp4\"\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\",\n",
    "        stream_url,\n",
    "        \"-t\",\n",
    "        str(record_duration),\n",
    "        \"-filter:v\",\n",
    "        \"crop=700:600:350:400\",\n",
    "        output_file,\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ffmpeg_command, timeout=record_duration + 10, check=True)\n",
    "    except Exception as e:\n",
    "        os.remove(output_file)\n",
    "        print(e)\n",
    "\n",
    "\n",
    "    print(f\"Recording completed. The video is saved as {output_file}.\")\n",
    "\n",
    "\n",
    "download_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(model, cap):\n",
    "    # Store the track history\n",
    "    track_history = defaultdict(lambda: [])\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Store the detected frames indices and speeds\n",
    "    frames_list = defaultdict(lambda: [])\n",
    "    speeds = defaultdict(lambda: [])\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "            results = model.track(frame, persist=True, classes=[0])\n",
    "\n",
    "            # Get the boxes and track IDs\n",
    "            boxes = results[0].boxes.xywh.cpu()\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "            # Visualize the results on the frame\n",
    "            annotated_frame = results[0].plot(conf=False)\n",
    "\n",
    "            # Plot the tracks\n",
    "            for box, track_id in zip(boxes, track_ids):\n",
    "                x, y, w, h = box\n",
    "                track = track_history[track_id]\n",
    "                track.append((float(x), float(y)))  # x, y center point\n",
    "                if len(track) > 120:  # retain 90 tracks for 90 frames\n",
    "                    track.pop(0)\n",
    "\n",
    "                frames = frames_list[track_id] # indices of frames in which the object was detected\n",
    "                frames.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "                if len(frames) > 2:\n",
    "                    frames.pop(0)\n",
    "\n",
    "                # Calculate the speed\n",
    "                if len(track) > 1:\n",
    "                    speed = np.linalg.norm(np.array(track[-1]) - np.array(track[-2])) * fps / (frames[-1] - frames[-2])\n",
    "                    speeds[track_id] = speed\n",
    "\n",
    "                    # Draw the speed in box title\n",
    "                    cv2.putText(annotated_frame, f\"{speed:.2f} px/s\", (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                (255, 255, 255), 1)\n",
    "\n",
    "                # Draw the tracking lines\n",
    "                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "            yield (annotated_frame, boxes, track_ids, speeds)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "video_path = \"data/20231108-190653_10s.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "output_video_name = \"result.mp4\"\n",
    "tmp_output_path = \"tmp_\" + output_video_name\n",
    "\n",
    "output_video = cv2.VideoWriter(\n",
    "    tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height)\n",
    ")\n",
    "\n",
    "for frame, boxes, track_ids, speeds in track(model, cap):\n",
    "    clear_output(wait=True)\n",
    "    cv2.imshow(\"YOLOv8 Tracking\", frame)\n",
    "    # sleep(1)\n",
    "\n",
    "    for box, track_id in zip(boxes, track_ids):\n",
    "        x, y, w, h = box\n",
    "        x, y = int(x), int(y)\n",
    "        print(\n",
    "            f\"Person {track_id} is at ({x}, {y}) \"\n",
    "            + (\n",
    "                f\"commuting at {speeds[track_id]:.2f} px/s\"\n",
    "                if track_id in speeds\n",
    "                else \"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    output_video.write(frame)\n",
    "\n",
    "output_video.release()\n",
    "\n",
    "if os.path.exists(output_video_name):\n",
    "    os.remove(output_video_name)\n",
    "\n",
    "subprocess.run(\n",
    "    [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\",\n",
    "        tmp_output_path,\n",
    "        \"-crf\",\n",
    "        \"18\",\n",
    "        \"-preset\",\n",
    "        \"veryfast\",\n",
    "        \"-hide_banner\",\n",
    "        \"-loglevel\",\n",
    "        \"error\",\n",
    "        \"-vcodec\",\n",
    "        \"libx264\",\n",
    "        output_video_name,\n",
    "    ]\n",
    ")\n",
    "os.remove(tmp_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
